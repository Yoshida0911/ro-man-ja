\section{結言}
本研究では，大規模言語モデル（VLM）を用いて環境画像および音響情報から局所的ナビゲーション方針を表すパラメータを推定し，その情報を局所的ナビゲーションモジュールに適応させる新たな提案手法を述べた．
従来のロボットナビゲーション手法は，センサによる幾何学的な障害物回避に焦点を当てており，周囲の人間の状態や社会的状況といった抽象的な文脈情報を正確に解釈し，ナビゲーションに効果的に統合することは困難であった．
しかし，本提案手法は，VLMの高度な統合的認識能力を活用し，視覚情報と音響情報，およびロボットに与えられた役割を紐づけることで，実環境においても機能する文脈依存のポテンシャル場パラメータを動的に生成することを可能にした．

宇都宮大学キャンパスにて実施した実機実験により，本手法に基づくナビゲーションが，幾何学的には同一の対象であっても，役割や音響情報の有無に応じて効率的な回避や支援のための接近といった適切な行動変容を実現し，より安全で社会規範に適合したナビゲーションをもたらすことを確認した．

今後の課題として，環境の動的変化に即応するための計算効率の向上とリアルタイムな統合手法の開発，およびVLMの推論に伴うパラメータ変動の安定化が挙げられる．
さらに，現在のシステムは音響情報の強度のみを利用しているが，今後は音声内容の理解を含めたマルチモーダルな文脈理解や，照明・路面状況などの多様な環境要因の統合手法を開発し，VLMを用いた抽象情報分析の精度と安定性に関するさらなる検証が必要である．